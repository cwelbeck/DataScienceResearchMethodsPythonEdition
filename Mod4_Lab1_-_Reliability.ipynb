{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Note to Python users:** As of this time, there are no Python packages with the required functionality to perform the reliability analysis discussed in this course. Therefore, this R notebook is being made available to you. Please read the contents of this notebook, execute the code, and answer the evaluation questions. \n\n> We hope to replace this lab as soon as the required functionality is available. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Module 4, Lab 1 - Reliability\n=============================\n\nIn this lab, we will explore the idea of reliability. Reliability is\nabout consistency in measurement, and it is a bare minimum in order for\na measure to be sound. After all, if you're not consistently measuring\nsomething, how can you say your measures are trustworthy?\n\nIn this lab, we will use the `psych` package for psychometrics (e.g.,\nreliability analysis)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#### LOAD DATA ####\ninstall.packages('psych')\nlibrary(psych)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Installing package into ‘/home/nbuser/R’\n(as ‘lib’ is unspecified)\nWarning message in install.packages(\"psych\"):\n“installation of package ‘psych’ had non-zero exit status”",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Imagine you are given a questionnaire developed by a the executives of a\ntaco company. They want to know how people feel about their brand. In a\nsurvey, 200 people were asked their impressions of the taco brand on\nfive adjectives representing core themes used in the branding process\nand agreed upon by the executive board of the company. These themes are\nmeant to be distinctives of the brand. Every participant rated how much\neach adjective describes the brand on a 1 (not at all) to 10\n(completely) scale. The company has suggested the participant score can\nform a \"brand value\" index.\n\n1.  Inviting\n2.  Friendly\n3.  Awesome\n4.  Quirky\n5.  Pleasant\n\nYou seek to know whether these questions reliably and consistently\nmeasure brand sentiment. To answer that question, we must first ask\nthree questions:\n\n1.  Are the answers to these questions interrelated\n2.  If so, there evidence that these items seem to be measuring \"one\n    thing\" or \"more than one thing\"\n3.  If they are measuring one thing, do they do so reliably and how can\n    we optimize reliability?\n\nStart by loading the data:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#### LOAD DATA ####\ndat <- read.csv(\"measurement.csv\")",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we should check the data:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "summary(dat)\n\n##        id            friendly         inviting         awesome      \n##  Min.   :  1.00   Min.   : 4.455   Min.   : 3.092   Min.   : 3.221  \n##  1st Qu.: 50.75   1st Qu.: 6.860   1st Qu.: 6.800   1st Qu.: 6.656  \n##  Median :100.50   Median : 7.960   Median : 7.873   Median : 8.215  \n##  Mean   :100.50   Mean   : 7.917   Mean   : 7.796   Mean   : 7.899  \n##  3rd Qu.:150.25   3rd Qu.: 9.080   3rd Qu.: 9.047   3rd Qu.: 9.255  \n##  Max.   :200.00   Max.   :10.000   Max.   :10.000   Max.   :10.000  \n##      quirky         pleasant     \n##  Min.   : 8.71   Min.   : 3.976  \n##  1st Qu.:10.00   1st Qu.: 6.832  \n##  Median :10.00   Median : 7.793  \n##  Mean   : 9.97   Mean   : 7.809  \n##  3rd Qu.:10.00   3rd Qu.: 9.167  \n##  Max.   :10.00   Max.   :10.000",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "       id            friendly         inviting         awesome      \n Min.   :  1.00   Min.   : 4.455   Min.   : 3.092   Min.   : 3.221  \n 1st Qu.: 50.75   1st Qu.: 6.860   1st Qu.: 6.800   1st Qu.: 6.656  \n Median :100.50   Median : 7.960   Median : 7.873   Median : 8.215  \n Mean   :100.50   Mean   : 7.917   Mean   : 7.796   Mean   : 7.899  \n 3rd Qu.:150.25   3rd Qu.: 9.080   3rd Qu.: 9.047   3rd Qu.: 9.255  \n Max.   :200.00   Max.   :10.000   Max.   :10.000   Max.   :10.000  \n     quirky         pleasant     \n Min.   : 8.71   Min.   : 3.976  \n 1st Qu.:10.00   1st Qu.: 6.832  \n Median :10.00   Median : 7.793  \n Mean   : 9.97   Mean   : 7.809  \n 3rd Qu.:10.00   3rd Qu.: 9.167  \n Max.   :10.00   Max.   :10.000  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The immediate question before us is whether these five adjectives\nreliably form a single index of anything.\n\nThe first task is to check the correlations among the items. To make the\ncode easier, we can briefly drop the `id` variable. All of the commands\nfrom here on out require that we input a data frame that has **only**\nthe items we wish to analyze. You might also make a new dataframe for\njust the items you want if you are working with a larger set of data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dat <- dat[,-1]\nnames(dat)\n\n## [1] \"friendly\" \"inviting\" \"awesome\"  \"quirky\"   \"pleasant\"",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "[1] \"friendly\" \"inviting\" \"awesome\"  \"quirky\"   \"pleasant\"",
            "text/latex": "\\begin{enumerate*}\n\\item 'friendly'\n\\item 'inviting'\n\\item 'awesome'\n\\item 'quirky'\n\\item 'pleasant'\n\\end{enumerate*}\n",
            "text/markdown": "1. 'friendly'\n2. 'inviting'\n3. 'awesome'\n4. 'quirky'\n5. 'pleasant'\n\n\n",
            "text/html": "<ol class=list-inline>\n\t<li>'friendly'</li>\n\t<li>'inviting'</li>\n\t<li>'awesome'</li>\n\t<li>'quirky'</li>\n\t<li>'pleasant'</li>\n</ol>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we can run `cor()` on the measures:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "round(cor(dat), 2)\n\n##          friendly inviting awesome quirky pleasant\n## friendly     1.00     0.81    0.80   0.30     0.81\n## inviting     0.81     1.00    0.70   0.22     0.64\n## awesome      0.80     0.70    1.00   0.23     0.64\n## quirky       0.30     0.22    0.23   1.00     0.27\n## pleasant     0.81     0.64    0.64   0.27     1.00",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "         friendly inviting awesome quirky pleasant\nfriendly 1.00     0.81     0.80    0.30   0.81    \ninviting 0.81     1.00     0.70    0.22   0.64    \nawesome  0.80     0.70     1.00    0.23   0.64    \nquirky   0.30     0.22     0.23    1.00   0.27    \npleasant 0.81     0.64     0.64    0.27   1.00    ",
            "text/latex": "A matrix: 5 x 5 of type dbl\n\\begin{tabular}{r|lllll}\n  & friendly & inviting & awesome & quirky & pleasant\\\\\n\\hline\n\tfriendly & 1.00 & 0.81 & 0.80 & 0.30 & 0.81\\\\\n\tinviting & 0.81 & 1.00 & 0.70 & 0.22 & 0.64\\\\\n\tawesome & 0.80 & 0.70 & 1.00 & 0.23 & 0.64\\\\\n\tquirky & 0.30 & 0.22 & 0.23 & 1.00 & 0.27\\\\\n\tpleasant & 0.81 & 0.64 & 0.64 & 0.27 & 1.00\\\\\n\\end{tabular}\n",
            "text/markdown": "\nA matrix: 5 x 5 of type dbl\n\n| <!--/--> | friendly | inviting | awesome | quirky | pleasant |\n|---|---|---|---|---|---|\n| friendly | 1.00 | 0.81 | 0.80 | 0.30 | 0.81 |\n| inviting | 0.81 | 1.00 | 0.70 | 0.22 | 0.64 |\n| awesome | 0.80 | 0.70 | 1.00 | 0.23 | 0.64 |\n| quirky | 0.30 | 0.22 | 0.23 | 1.00 | 0.27 |\n| pleasant | 0.81 | 0.64 | 0.64 | 0.27 | 1.00 |\n\n",
            "text/html": "<table>\n<caption>A matrix: 5 x 5 of type dbl</caption>\n<thead>\n\t<tr><th></th><th scope=col>friendly</th><th scope=col>inviting</th><th scope=col>awesome</th><th scope=col>quirky</th><th scope=col>pleasant</th></tr>\n</thead>\n<tbody>\n\t<tr><th scope=row>friendly</th><td>1.00</td><td>0.81</td><td>0.80</td><td>0.30</td><td>0.81</td></tr>\n\t<tr><th scope=row>inviting</th><td>0.81</td><td>1.00</td><td>0.70</td><td>0.22</td><td>0.64</td></tr>\n\t<tr><th scope=row>awesome</th><td>0.80</td><td>0.70</td><td>1.00</td><td>0.23</td><td>0.64</td></tr>\n\t<tr><th scope=row>quirky</th><td>0.30</td><td>0.22</td><td>0.23</td><td>1.00</td><td>0.27</td></tr>\n\t<tr><th scope=row>pleasant</th><td>0.81</td><td>0.64</td><td>0.64</td><td>0.27</td><td>1.00</td></tr>\n</tbody>\n</table>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see here that most of the items are strongly inter-correlated, except\nfor `quirky`. This question doesn't' seem to fit with the others. We may\nflag this as a possible odd item.\n\nOne Dimension?\n==============\n\nWe can break down the reliability test into two stages. In the first\nstage, we can ask whether we are measuring \"one thing\" or several things\nwith this set of questions. *After all, you cannot have a reliable\nmeasure of something unless you know you are measuring one thing to\nbegin with.* Although many people skip this step, I illustrate it here.\n\nA factor analysis (similar to a \"principal component analysis,\" which\nyou will learn later) can tell us how many underlying dimensions appear\nto be operating beneath this set of variables. For example, if I asked\nsomeone feeling angry if they feel \"upset,\" \"mad,\" and \"angry\" ... I\nwould likely get similar answers because these questions are all\nassessing the **one** underlying dimension of anger. There does not\nappear to be a separate dimension beneath those items (we call questions\n\"items\" in survey writing). Similarly, we need to know if this set of\nquestions we have asked represents one underlying dimension or several.\nWe call these underlying dimensions \"factors.\" Thus, we need to know how\nmany factors we have.\n\nI won't explain the details of factor analysis here (beyond the scope of\nthis course), but it essentially analyzes the patterns of association\nbetween the items to determine if they \"cluster together\" and if so,\nhow. We start with `fa.parallel()` from the `psych` package. We tell it\nwe want to use the `minres` factor method (`fm = 'minres'`), which makes\nfew assumptions about the normality of the data, and we tell it we want\nthe analysis to be a \"factor analysis\" (`fa = 'fa'`).\n\nI won't go into details about the analysis, but we compute a number of\n\"eigenvalues\" for our data. Historically, the number of eigenvalues\nabove 1.0 was seen as the number of factors in the data. For example, if\nwe had 3 eigenvalues above 1.0, that would be evidence that our\nquestions measured 3 underlying dimensions.\n\nModern techniques involve a \"parallel analysis,\" in which the computer\ngenerates random data (based on your data) but with *no* underlying\nfactors. Where your data differ from the simulated data, you have\nevidence for a factor. We count the number of times that occurs, and\nthat is the number of factors you have."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "fa.parallel(dat, fm = 'minres', fa = 'fa')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![](Mod4_Lab1_-_Reliability_files/figure-markdown_strict/unnamed-chunk-6-1.png)"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## Parallel analysis suggests that the number of factors =  1  and the number of components =  NA",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The computer here is telling you that we see evidence for one factor.\nThis is supported by the graph. The red lines in the graph represent the\nresults from the random computer-generated data, whereas the blue line\nrepresents your data. We see that, in the column labeled \"Factor Number\n1,\" the blue line and red lines diverge--evidence for a factor! Further,\nthe \"eigenvalue\" for that column is above 1.0 (height of blue triangle\non the y-axis).\n\nFor the remaining columns on the graph, we see that our data (blue line)\ngives identical results to the simulated random data (red lines). Thus,\nwe have no evidence for any additional factors. Therefore, we can\nconclude that we have one underlying dimension to our data. This means\nwe are measuring \"one thing\" with this set of questions.\n\nNext, we can can focus in on our questions and consider whether our\nitems each correlate well with the \"one underlying dimension.\" A good\nquestion is conceptually close to the underlying dimension and therefore\ndoes not \"pick up\" on a lot of other junk. For example, the \"quirky\"\nquestion has a possible negative connotation to it; it may inadvertently\nhave more going on with it than simply assessing positive / negative\nsentiment. Thus, I would not be surprised if it fails to correlate as\nwell with the underlying factor. We call these correlations \"loadings.\"\n\nWe can run this more detailed factor analysis using the `fa()` command\nfrom the `psych` package, specifically asking it to retrieve one factor.\nWe then look at the \"loadings.\" Good questions should be highly\ncorrelated to our underlying dimension. In general, I would be wary of\nanything in the .3-.5 range or below. These loading are given in the\ncolumn labeled `MR1`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "fa(dat, nfactors = 1, fm=\"minres\")$loadings\n\n## \n## Loadings:\n##          MR1  \n## friendly 0.992\n## inviting 0.818\n## awesome  0.812\n## quirky   0.299\n## pleasant 0.805\n## \n##                  MR1\n## SS loadings    3.048\n## Proportion Var 0.610",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "(Note: you can get much more detailed analyses if you omit the\n`$loadings` code, but you won't be able to make sense of it without a\nricher understanding of factor analysis, which is beyond this tutorial).\n\nWe see here that the \"friendly\", \"inviting,\" \"awesome,\" and \"pleasant\"\nquestions are highly correlated (~.80) with the underlying factor. This\nis good news, because these questions are tracking closely with the\nunderlying dimension. However, not surprisingly, \"quirky\" does not load\nas well. In fact, the correlation is weak, loading at only .299. At this\npoint, I would consider analyzing quirky separately or removing it from\nthe measure entirely.\n\nThere is one point worth mentioning. Factor analyses almost always\noverfit the data (i.e., they will model the sample data perfectly).\nThus, they are often worth replicating in a separate set of data, if\npossible. It's sometimes useful to split the data in half, for example,\ntraining the model in one set of data and cross-validating in the other.\n\nReliability\n===========\n\nNext, we can assess the reliability of the measure. Technically defined,\nreliability is \"the percentage of variance in the scores due to the\nthing you're measuring\" (i.e., due to the factor). If reliability is\n100%, then our measure is perfectly reliable and every bump in the data\nis real variation in sentiment. If reliability is less than 100%--for\nexample, 80%--then most of the variation is due to the thing you are\nmeasuring but the remaining 20% is measurement error, other random\nunrelated junk that you do not care about and is watering down your\nmeasure.\n\nI will point out that there are trade-offs in measurement. You can get a\nperfectly reliable measure by asking the same question several different\nways, but that is redundant and frankly uninformative. It's worth it to\nask questions slightly differently (e.g., words such as \"friendly\" and\n\"awesome\" are both positive words but not identical). You add more\ninformation to your measure that way, but the trade off is that\nreliability can suffer. In general, reliability between .70 and .90 is\ngood, with .80-.95 preferred.\n\nTo run our test, we use `alpha()` from the `psych` package. Since both\n`ggplot2` and `psych` have commands with that name, it's useful to\npreface the command with `psych::` to be sure that it pulls the command\nfrom `psych`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "psych::alpha(dat)\n\n## \n## Reliability analysis   \n## Call: psych::alpha(x = dat)\n## \n##   raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd\n##       0.86      0.86    0.87      0.54 5.9 0.0098  8.3 1.1\n## \n##  lower alpha upper     95% confidence boundaries\n## 0.84 0.86 0.88 \n## \n##  Reliability if an item is dropped:\n##          raw_alpha std.alpha G6(smc) average_r  S/N alpha se\n## friendly      0.77      0.77    0.75      0.45  3.3    0.016\n## inviting      0.80      0.81    0.82      0.51  4.1    0.012\n## awesome       0.81      0.81    0.82      0.51  4.1    0.012\n## quirky        0.91      0.92    0.91      0.73 10.9    0.011\n## pleasant      0.81      0.80    0.82      0.51  4.1    0.011\n## \n##  Item statistics \n##            n raw.r std.r r.cor r.drop mean   sd\n## friendly 200  0.95  0.93  0.96   0.92  7.9 1.35\n## inviting 200  0.88  0.85  0.82   0.78  7.8 1.54\n## awesome  200  0.88  0.84  0.81   0.77  7.9 1.70\n## quirky   200  0.31  0.51  0.30   0.29 10.0 0.14\n## pleasant 200  0.87  0.85  0.82   0.76  7.8 1.61",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The top row gives us what we want:\n\n`raw_alpha`  \n`0.86`\n\nOur measure is an estimated 86% reliable, or only 14% measurement error.\nThis row also tells us that the average correlation among our items is\n.54. Can we do better?\n\nHowever, this included `quirky`. We see below the top row is the line,\n`Reliability if an item is dropped:` which reports that dropping\n`quirky` would actually improve our reliability to .91. This row also\ntells us that the average correlation among our questions would increase\nto .73.\n\nThis confirms our gut intuition, our factor analysis, and now our\nreliability analysis. I would drop the quirky item (and report it\nseparately if desired).\n\nYou can quickly score the scale using `rowSums()`, which accepts a\n`data.frame()` of the desired variables."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "dat$sentiment <- rowSums(data.frame(dat$friendly, dat$inviting, dat$awesome, dat$pleasant))\n\nhist(dat$sentiment)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![](Mod4_Lab1_-_Reliability_files/figure-markdown_strict/unnamed-chunk-9-1.png)\n\nWe still have not assessed whether the thing we are measuring *is*\nsentiment. For example, maybe it's just measuring enthusiasm in filling\nout survey questions! Still, it's a start. We can next explore the\nvalidity of the measure, which we will do in the next lab.\n\nFuture Directions\n=================\n\nThis is but a first taste of measurement development. There is an entire\nfield known as \"psychometrics\" with many fantastic tools for measurement\ndevelopment. Good measurement is hard, but without good measurement, our\ndata are wrong. Further, some cases misleading data can be worse than no\ndata. Taking the time to assess the reliability and validity of measures\nis therefore vital to doing good data science whenever measures must be\nconstructed."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "r",
      "display_name": "R",
      "language": "R"
    },
    "language_info": {
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.5.3",
      "file_extension": ".r",
      "codemirror_mode": "r"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}